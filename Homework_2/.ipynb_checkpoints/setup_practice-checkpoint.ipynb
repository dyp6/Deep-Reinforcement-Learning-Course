{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maml_env import HalfCheetahDirecBulletEnv\n",
    "import tensorflow as tf\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.specs import array_spec\n",
    "import tensorflow_probability as tfp\n",
    "from tf_agents.policies import greedy_policy\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as keras_backend\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.policies import actor_policy\n",
    "import tensorflow.keras.losses as kls\n",
    "from tf_agents.metrics import tf_metrics\n",
    "keras_backend.set_floatx('float32')\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tf_agents.networks import network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tasks:\n",
    "    def __init__(self, *task_configs):\n",
    "        self.tasks = [i for i in task_configs]\n",
    "\n",
    "    def sample_tasks(self, batch_size):\n",
    "        return random.choices(self.tasks, k=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class policyNet(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = keras.layers.Dense(40, activation=\"relu\",input_shape=(1,),name = \"Inner Net Input\")\n",
    "        self.hidden2 = keras.layers.Dense(40, activation = \"relu\", name = \"Inner Net Hidden\")\n",
    "        self.out = keras.layers.Dense(6,activation=\"tanh\", name = \"Inner Net Ouput\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        output = self.hidden1(x)\n",
    "        output = self.hidden2(output)\n",
    "        output = self.out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNet(network.Network):\n",
    "\n",
    "    def __init__(self, input_tensor_spec, output_tensor_spec):\n",
    "        super(ActionNet, self).__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),name='ActionNet')\n",
    "        self._output_tensor_spec = output_tensor_spec\n",
    "        self._sub_layers = [\n",
    "            tf.keras.layers.Dense(26, activation = tf.nn.relu),\n",
    "            tf.keras.layers.Dense(30,activation = tf.nn.relu),\n",
    "            tf.keras.layers.Dense(\n",
    "                action_spec.shape.num_elements(), activation=tf.nn.tanh),\n",
    "    ]\n",
    "    \n",
    "    def call(self, observations, step_type=(),network_state=()):\n",
    "\n",
    "        output = tf.cast(observations, dtype=tf.float32)\n",
    "        \n",
    "        for layer in self._sub_layers:\n",
    "            output = layer(output)\n",
    "        \n",
    "        actions = tf.reshape(output, [-1] + self._output_tensor_spec.shape.as_list())\n",
    "        return actions, network_state\n",
    "\n",
    "class ValueNet(network.Network):\n",
    "\n",
    "    def __init__(self, input_tensor_spec, output_tensor_spec):\n",
    "        super(ValueNet, self).__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),name='ValueNet')\n",
    "        self._output_tensor_spec = output_tensor_spec\n",
    "        self._sub_layers = [\n",
    "            tf.keras.layers.Dense(26, activation = tf.nn.relu),\n",
    "            tf.keras.layers.Dense(30,activation = tf.nn.relu),\n",
    "            tf.keras.layers.Dense(1,activation=None),\n",
    "    ]\n",
    "    \n",
    "    def call(self, observations, step_type=(),network_state=()):\n",
    "\n",
    "        output = tf.cast(observations, dtype=tf.float32)\n",
    "        \n",
    "        for layer in self._sub_layers:\n",
    "            output = layer(output)\n",
    "        \n",
    "        values = tf.reshape(output, [-1] + self._output_tensor_spec.shape.as_list())\n",
    "        return values, network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionDistributionNet(ActionNet):\n",
    "    def call(self, observations):\n",
    "        action_means, network_state = super(ActionDistributionNet, self).call(\n",
    "                observations)\n",
    "\n",
    "        action_std = tf.Variable(tf.ones_like(action_means),dtype=tf.float32,name=\"Inner Sigma\")\n",
    "        return tfp.distributions.MultivariateNormalDiag(action_means, action_std), network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self,input_tensor_spec,action_spec,value_specs):\n",
    "        self.input_tensor_spec = input_tensor_spec\n",
    "        self.action_spec = action_spec\n",
    "        self.value_specs = value_specs\n",
    "        self.policy = ActionDistributionNet(self.input_tensor_spec,self.action_spec)\n",
    "        self.critic = ValueNet(self.input_tensor_spec,self.value_specs)\n",
    "        self.a_opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "        self.c_opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "        self.clip_pram = 0.2\n",
    "        \n",
    "    def act(self,state):\n",
    "        dist,_ = self.policy(state)\n",
    "        action = dist.sample()\n",
    "        mult = tf.constant(2.,dtype=tf.float32)\n",
    "        sub = tf.constant(1.,dtype=tf.float32)\n",
    "        action = tf.subtract(\n",
    "                    tf.multiply(mult,\n",
    "                        tf.divide(\n",
    "                            tf.subtract(\n",
    "                                action, \n",
    "                                tf.reduce_min(action)\n",
    "                                ), \n",
    "                            tf.subtract(\n",
    "                                tf.reduce_max(action), \n",
    "                                tf.reduce_min(action)\n",
    "                                )\n",
    "                            )\n",
    "                        ),sub\n",
    "                            \n",
    "                    )\n",
    "        return action\n",
    "            \n",
    "    def learn_pg(self,states,actions,rewards):\n",
    "        with tf.GradientTape() as tape:\n",
    "            dist , _ = self.policy(states)\n",
    "            rewards = tf.reshape(rewards,(len(rewards),1))\n",
    "            actions = tf.reshape(actions,(len(actions),6))\n",
    "            logps = dist.log_prob(actions)\n",
    "            loss = tf.math.negative(tf.reduce_sum(tf.math.multiply(rewards,logps)))\n",
    "            \n",
    "        grads = tape.gradient(loss,self.policy.trainable_variables)\n",
    "        self.a_opt.apply_gradients(zip(grads,self.policy.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    def preprocess_ppo(self, states, actions, rewards, values):\n",
    "        g = 0\n",
    "        lmbda = 1\n",
    "        returns = []\n",
    "        new_vals = []\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            if i+1 % 201 == 0:\n",
    "                pass\n",
    "            delta = rewards[i] + values[i + 1] - values[i]\n",
    "            g = delta + lmbda * g\n",
    "            returns.append(g + values[i])\n",
    "            new_vals.append(values[i])\n",
    "\n",
    "        returns.reverse()\n",
    "        adv = np.array(returns, dtype=np.float32) - new_vals\n",
    "        adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-10)\n",
    "        states = np.array(states, dtype=np.float32)\n",
    "        actions = np.array(actions, dtype=np.float32)\n",
    "        returns = np.array(returns, dtype=np.float32)\n",
    "        return states, actions, returns, adv\n",
    "    \n",
    "    def learn_ppo(self, states, actions,  adv , old_probs, discnt_rewards):\n",
    "        discnt_rewards = tf.reshape(discnt_rewards, (len(discnt_rewards),))\n",
    "        adv = tf.reshape(adv, (len(adv),))\n",
    "\n",
    "        old_p = old_probs\n",
    "\n",
    "        old_p = tf.reshape(old_p, (len(old_p),1))\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            dist, _ = self.policy(states)\n",
    "            p = dist.log_prob(actions)\n",
    "            entropy = tf.math.negative(tf.reduce_sum(tf.math.multiply(discnt_rewards,p)))\n",
    "            sur1 = []\n",
    "            sur2 = []\n",
    "            for pb, t, op in zip(probs, adv, old_probs):\n",
    "                        t =  tf.constant(t)\n",
    "                        op =  tf.constant(op)\n",
    "                        ratio = tf.math.divide(pb,op)\n",
    "                        s1 = tf.math.multiply(ratio,t)\n",
    "                        s2 =  tf.math.multiply(tf.clip_by_value(ratio, 1.0 - self.clip_pram, 1.0 + self.clip_pram),t)\n",
    "                        sur1.append(s1)\n",
    "                        sur2.append(s2)\n",
    "            sr1 = tf.stack(sur1)\n",
    "            sr2 = tf.stack(sur2)\n",
    "            v, _ = self.critic(states)\n",
    "            v = tf.reshape(v, (len(v),1))\n",
    "            td = tf.math.subtract(discnt_rewards, v)\n",
    "            c_loss = 0.5 * kls.mean_squared_error(discnt_rewards, v)\n",
    "            a_loss = tf.math.negative(tf.reduce_mean(tf.math.minimum(sr1, sr2)) - c_loss + 0.001 * entropy)\n",
    "            \n",
    "        grads1 = tape1.gradient(a_loss, self.policy.trainable_variables)\n",
    "        grads2 = tape2.gradient(c_loss, self.critic.trainable_variables)\n",
    "        self.a_opt.apply_gradients(zip(grads1, self.policy.trainable_variables))\n",
    "        self.c_opt.apply_gradients(zip(grads2, self.critic.trainable_variables))\n",
    "        return a_loss, c_loss\n",
    "    \n",
    "    def actor_loss_ppo(self, probs, actions, adv, old_probs, closs):\n",
    "        \n",
    "        probability = probs      \n",
    "        entropy = tf.reduce_mean(tf.math.negative(tf.math.multiply(probability,tf.math.log(probability))))\n",
    "        sur1 = []\n",
    "        sur2 = []\n",
    "        \n",
    "        for pb, t, op in zip(probability, adv, old_probs):\n",
    "            t =  tf.constant(t)\n",
    "            op =  tf.constant(op)\n",
    "            ratio = tf.math.divide(pb,op)\n",
    "            s1 = tf.math.multiply(ratio,t)\n",
    "            s2 =  tf.math.multiply(tf.clip_by_value(ratio, 1.0 - self.clip_pram, 1.0 + self.clip_pram),t)\n",
    "            sur1.append(s1)\n",
    "            sur2.append(s2)\n",
    "\n",
    "        sr1 = tf.stack(sur1)\n",
    "        sr2 = tf.stack(sur2)\n",
    "        \n",
    "        loss = tf.math.negative(tf.reduce_mean(tf.math.minimum(sr1, sr2)) - closs + 0.001 * entropy)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_actor(policy, x, input_tensor_spec,action_spec):\n",
    "    '''Copy model weights to a new model.\n",
    "    \n",
    "    Args:\n",
    "        model: model to be copied.\n",
    "        x: An input example. This is used to run\n",
    "            a forward pass in order to add the weights of the graph\n",
    "            as variables.\n",
    "    Returns:\n",
    "        A copy of the model.\n",
    "    '''\n",
    "    policy(tf.convert_to_tensor(x))\n",
    "    copied_model_actor = ActionDistributionNet(input_tensor_spec,action_spec)\n",
    "    # If we don't run this step the weights are not \"initialized\"\n",
    "    # and the gradients will not be computed.\n",
    "    copied_model_actor(tf.convert_to_tensor(x))\n",
    "   \n",
    "    copied_model_actor.set_weights(policy.get_weights())\n",
    "    \n",
    "    return copied_model_actor\n",
    "\n",
    "def copy_critic(critic, x, input_tensor_spec,value_spec):\n",
    "    '''Copy model weights to a new model.\n",
    "    \n",
    "    Args:\n",
    "        model: model to be copied.\n",
    "        x: An input example. This is used to run\n",
    "            a forward pass in order to add the weights of the graph\n",
    "            as variables.\n",
    "    Returns:\n",
    "        A copy of the model.\n",
    "    '''\n",
    "    critic(tf.convert_to_tensor(x))\n",
    "    copied_model_critic = ValueNet(input_tensor_spec,value_spec)\n",
    "    # If we don't run this step the weights are not \"initialized\"\n",
    "    # and the gradients will not be computed.\n",
    "    copied_model_critic(tf.convert_to_tensor(x))\n",
    "   \n",
    "    copied_model_critic.set_weights(critic.get_weights())\n",
    "    \n",
    "    return copied_model_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doug/anaconda3/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n",
      "Meta Iteration Complete\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-aa1608355cc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mnew_Rs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mactor_dist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value, name, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m     \"\"\"\n\u001b[0;32m--> 964\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36m_call_log_prob\u001b[0;34m(self, value, name, **kwargs)\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_and_control_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_log_prob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_prob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_probability/python/internal/distribution_util.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_probability/python/distributions/mvn_linear_operator.py\u001b[0m in \u001b[0;36m_log_prob\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    226\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdistribution_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAppendDocstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_mvn_sample_note\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultivariateNormalLinearOperator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdistribution_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAppendDocstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_mvn_sample_note\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py\u001b[0m in \u001b[0;36m_log_prob\u001b[0;34m(self, y, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mprefer_static\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank_from_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_shape_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverride_event_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_event_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         self.event_shape)\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m     ildj = self.bijector.inverse_log_det_jacobian(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36mevent_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    872\u001b[0m     \"\"\"\n\u001b[1;32m    873\u001b[0m     return nest.map_structure_up_to(\n\u001b[0;32m--> 874\u001b[0;31m         self.dtype, tf.TensorShape, self._event_shape(), check_types=False)\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mis_scalar_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'is_scalar_event'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py\u001b[0m in \u001b[0;36m_event_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    337\u001b[0m           self._override_event_shape)\n\u001b[1;32m    338\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_event_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36mevent_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    872\u001b[0m     \"\"\"\n\u001b[1;32m    873\u001b[0m     return nest.map_structure_up_to(\n\u001b[0;32m--> 874\u001b[0;31m         self.dtype, tf.TensorShape, self._event_shape(), check_types=False)\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mis_scalar_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'is_scalar_event'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_probability/python/distributions/sample.py\u001b[0m in \u001b[0;36m_event_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_event_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_static_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtensorshape_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m       \u001b[0msample_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_probability/python/distributions/sample.py\u001b[0m in \u001b[0;36msample_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_batch_shape_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Sample one trajectory for task_i and collect the transitions (old_state,action,new_state,reward)\n",
    "tasks = Tasks((\"Forward\", True), (\"Backward\", False))\n",
    "task_config = tasks.sample_tasks(1)\n",
    "task_name, env_args = task_config[0], task_config[1:]\n",
    "env = HalfCheetahDirecBulletEnv(*env_args)\n",
    "input_tensor_spec = tensor_spec.TensorSpec((26,), tf.float32)\n",
    "action_spec = tensor_spec.BoundedTensorSpec((6,),tf.float32,minimum=-1,maximum=1)\n",
    "value_spec = tensor_spec.TensorSpec((1,),tf.float32)\n",
    "# Instantiate initial agent with random policy theta\n",
    "outer_agent = agent(input_tensor_spec,action_spec,value_spec)\n",
    "# Update temp outer agent with trajectories sampled from each updated policy in env. t_i\n",
    "temp_outer_agent = agent(input_tensor_spec,action_spec,value_spec)\n",
    "K = 1\n",
    "meta_iterations = 100\n",
    "num_adapt_steps=0\n",
    "# Sample tasks from task distribution to train policy on\n",
    "avg_meta_return=[]\n",
    "meta_iter_policies=[]\n",
    "meta_iter_critics=[]\n",
    "task_list = []\n",
    "for meta_iter in range(meta_iterations):\n",
    "    # Update actual outer agent policy with temp outer agent after learning with theta`_i from each task\n",
    "    if meta_iter_policies:\n",
    "        outer_agent.critic= meta_iter_critics[-1]\n",
    "        outer_agent.policy=meta_iter_policies[-1]\n",
    "    avg_task_return = []\n",
    "    for task_config in tasks.sample_tasks(20):\n",
    "        task_name, env_args = task_config[0], task_config[1:]\n",
    "        task_list.append(task_name)\n",
    "        env = HalfCheetahDirecBulletEnv(*env_args)\n",
    "        # Create a copy of the model to sample trajectories with new policy theta'\n",
    "        actor_copy = copy_actor(outer_agent.policy,env.reset().reshape(1,26),\n",
    "                                            input_tensor_spec,action_spec)\n",
    "        # Create the inner agent to update the policy using the copy of the model made above\n",
    "        inner_agent = agent(input_tensor_spec,action_spec,value_spec)\n",
    "        inner_agent.policy=actor_copy\n",
    "        if num_adapt_steps==1 | num_adapt_steps==2:\n",
    "            # Sample K trajectories from outer policy\n",
    "            Ss = []\n",
    "            As = []\n",
    "            Rs = []\n",
    "            for _ in range(K):\n",
    "                state = env.reset()\n",
    "                h = 0 \n",
    "                # Sample K trajectories from initial policy on task_i\n",
    "                while True:\n",
    "                    Ss.append(state)\n",
    "                    action = outer_agent.act(state.reshape(1,26))\n",
    "                    As.append(action)\n",
    "                    state, reward, done, _ = env.step(action.numpy().flatten())\n",
    "                    Rs.append(reward)\n",
    "                    h += 1\n",
    "                    if h == 200:\n",
    "                        break\n",
    "            # Calculate the loss and update the inner agent\n",
    "            task_loss = inner_agent.learn_pg(np.array(Ss,dtype=np.float32),As,np.array(Rs,dtype=np.float32))\n",
    "        if num_adapt_steps==2:\n",
    "            # Sample K trajectories from outer policy\n",
    "            Ss = []\n",
    "            As = []\n",
    "            Rs = []\n",
    "            for _ in range(K):\n",
    "                state = env.reset()\n",
    "                h = 0 \n",
    "                # Sample K trajectories from initial policy on task_i\n",
    "                while True:\n",
    "                    Ss.append(state)\n",
    "                    action = outer_agent.act(state.reshape(1,26))\n",
    "                    As.append(action)\n",
    "                    state, reward, done, _ = env.step(action.numpy().flatten())\n",
    "                    Rs.append(reward)\n",
    "                    h += 1\n",
    "                    if h == 200:\n",
    "                        break\n",
    "            task_loss2 = inner_agent.learn_pg(np.array(Ss,dtype=np.float32),As,np.array(Rs,dtype=np.float32))\n",
    "        # Sample trajectories using new policy theta' with inner agent\n",
    "        avg_return = []\n",
    "        new_Ss = []\n",
    "        new_As = []\n",
    "        new_Rs = []\n",
    "        values = []\n",
    "        probs = []\n",
    "        for _ in range(K):\n",
    "            state = env.reset()\n",
    "            h = 0\n",
    "            init_value,_ = inner_agent.critic(state.reshape(1,26))\n",
    "            values.append(init_value.numpy()[0])\n",
    "            # Sample K trajectories from updated policy on task_i\n",
    "            while True:\n",
    "                new_Ss.append(state)\n",
    "                action = inner_agent.act(state.reshape(1,26))\n",
    "                new_As.append(action)\n",
    "                state, reward, done, _ = env.step(action.numpy().flatten())\n",
    "                new_Rs.append(reward)\n",
    "                actor_dist,_ = inner_agent.policy(state.reshape(1,26))\n",
    "                probs.append(actor_dist.log_prob(action))\n",
    "                value,_ = inner_agent.critic(state.reshape(1,26))\n",
    "                values.append(value.numpy()[0])\n",
    "                h += 1\n",
    "                if h == 200:\n",
    "                    avg_return.append(sum(new_Rs[-200:]))\n",
    "                    break\n",
    "        states, actions,returns, adv  = inner_agent.preprocess_ppo(new_Ss, new_As, new_Rs, values)\n",
    "        \n",
    "        # Update temp outer agent with trajectories sampled from each updated policy in env. t_i\n",
    "        temp_outer_agent = agent(input_tensor_spec,action_spec,value_spec)\n",
    "        temp_outer_agent.policy=actor_copy\n",
    "        temp_outer_agent.critic=(copy_critic(outer_agent.critic,env.reset().reshape(1,26),\n",
    "                                              input_tensor_spec,value_spec))\n",
    "        actor_loss,critic_loss = temp_outer_agent.learn_ppo(np.array(new_Ss,dtype=np.float32),\n",
    "                                                            new_As,adv,probs,np.array(new_Rs,dtype=np.float32))\n",
    "        avg_task_return.append(sum(avg_return)/len(avg_return))\n",
    "    # Update outer agent policy without in a copy of the model, so the inner_agent isn't affected\n",
    "    meta_iter_critics.append(copy_critic(temp_outer_agent.critic,env.reset().reshape(1,26),input_tensor_spec,value_spec))\n",
    "    meta_iter_policies.append(copy_actor(temp_outer_agent.policy,env.reset().reshape(1,26),input_tensor_spec,value_spec))\n",
    "    avg_meta_return.append(sum(avg_task_return)/len(avg_task_return))\n",
    "    print(\"Meta Iteration Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    tasks = Tasks((\"Forward\", True), (\"Backward\", False))\n",
    "    input_tensor_spec = tensor_spec.TensorSpec((26,), tf.float32)\n",
    "    action_spec = tensor_spec.BoundedTensorSpec((6,),tf.float32,minimum=-1,maximum=1)\n",
    "    # Outer loop\n",
    "    for meta_iter in range(args.meta_iteration):\n",
    "        for task_config in tasks.sample_tasks(args.meta_batch_size):\n",
    "            # Inner loop\n",
    "            task_name, env_args = task_config[0], task_config[1:]\n",
    "            env = HalfCheetahDirecBulletEnv(*env_args)\n",
    "            init = env.reset()\n",
    "            # Adaptation\n",
    "\n",
    "            # Run adapted policy\n",
    "\n",
    "        # Meta Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--meta_iteration\", default=500, type=int)\n",
    "    parser.add_argument(\"--meta_batch_size\", default=40, type=int)\n",
    "    parser.add_argument(\"--horizon\", \"-H\", default=200, type=int)\n",
    "    parser.add_argument(\"--num_adapt_steps\", default=1, type=int)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
